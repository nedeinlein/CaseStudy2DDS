---
title: "CaseStudy2DDS"
Author: "Nathan Deinlein"
output: 
  html_document: 
    keep_md: yes
---
Youtube Link https://youtu.be/ihfd1bgg8xc

# Introduction
## The Problem Question
For this case study I was given a single data set from a fictional client and asked to look into what might cause an employee to leave a company. This term is known as attrition. The "client" asked to determine the three leading predictors of this event from their data. If a model could be created from this, could it accurately predict the event, and finally, as a side question, based on the data could a regression model be created to accurately predict the salaries of employees?

## The Process
* EDA - (Exploratory Data Analysis)
* Modeling for Attrition
* Predicting Attrition in a Test Set
* Modeling Regression for Salary
* Predicting Salary in a Test Set

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message= FALSE)
#environment
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
library(e1071)
library(caret)
library(plotly)
library(Metrics)
library(curl)
```

# Data Import and configuration
The first task was importing the data and changing the Attrition variable to a factor from a string. This allowed for classification in the models tested later.

```{r data import and summary}
#Base data import
base <- read.csv(curl("https://raw.githubusercontent.com/nedeinlein/CaseStudy2DDS/main/BaseData.csv"))

base$Attrition <- as.factor(base$Attrition)
df <- base
summary(df)
```

## Initial EDA
Rather than typing out every combination of plot needed for the EDA of this case study, I built an app to allow me to change between plot type and variables as I needed. The App can be found at https://nedeinlein.shinyapps.io/EDA_App/

## Modeling KNN (K Nearest Neighbors)
After looking at the data visually, the first model I wanted to test was a simple KNN model. KNN is a  machine learning tool that maps all the variables in the model, then uses the Euclidean (geometric) distance to determine the nearest neighbors to the point you are trying to categorize. It then uses a number, K, of these neighbors to "vote" on how a test point should be categorized. Because many of the variables did not appear to have stark differences between what could cause attrition and what would not, I worked on an original model with all continuous variables and subtracted until I came up with the final model. For this reason the initial model was used to try to find a base line K value to start with for further refining.
```{r KNN test of model}
#Set seed and building test and train data sets
set.seed(6)
splitPerc = .75
trainIndices = sample(1:dim(df)[1],round(splitPerc * dim(df)[1]))
train = df[trainIndices,]
test = df[-trainIndices,]

#finding most accurate K value for each continuous variable column (2,5,7,8,12,14,15,16,18,20,21,22,25,27,29,30,31,32,33,34,35,36)
accs = data.frame(accuracy = numeric(40), k = numeric(40))
for(i in 1:40)
{
  classifications = knn(train[,c(2,5,7,8,12,14,15,16,18,20,21,22,25,27,29,30,31,32,33,34,35,36)],test[,c(2,5,7,8,12,14,15,16,18,20,21,22,25,27,29,30,31,32,33,34,35,36)],train$Attrition, prob = TRUE, k = i)
  table(test$Attrition,classifications)
  CM = confusionMatrix(table(test$Attrition,classifications),positive = "Yes")
  accs$accuracy[i] = CM$overall[1]
  accs$sensitivity[i] = CM$byClass[1]
  accs$specificity[i] = CM$byClass[2]
  accs$k[i] = i
}

par(mfrow = c(2,2))
plot(accs$k,accs$accuracy, type = "l", xlab = "K")
plot(accs$k,accs$sensitivity, type = "l", xlab = "K")
plot(accs$k,accs$specificity, type = "l", xlab = "K")
```

## Improving Accuracy
After running this, the most accurate model that the sum of the data could create with all variables was at K = 11. This put the sensitivity level at 60%, specificity at around 85%, and accuracy at 85%. To improve the model, I then started removing variables. Variables that caused a large drop in accuracy were kept, and, with each run, the model was then checked to see if the drop was due to the variable or if it was simply an issue of needing to change the K value to something other than 11.

```{r KNN accuracy improvement}
#knn with all factors at k = 11 has a max sensitivity of 60%
classifications = knn(train[,c(2,5,7,8,12,14,15,16,18,20,21,22,25,27,29,30,31,32,33,34,35,36)],test[,c(2,5,7,8,12,14,15,16,18,20,21,22,25,27,29,30,31,32,33,34,35,36)],train$Attrition, prob = TRUE, k = 11)
table(test$Attrition,classifications)
confusionMatrix(table(test$Attrition,classifications),positive = "Yes")

#finding most accurate K value for after dropping variables
accs = data.frame(accuracy = numeric(40), k = numeric(40))
for(i in 1:40)
{
  classifications = knn(train[,c(5,20,21)],test[,c(5,20,21)],train$Attrition, prob = TRUE, k = i)
  table(test$Attrition,classifications)
  CM = confusionMatrix(table(test$Attrition,classifications),positive = "Yes")
  accs$accuracy[i] = CM$overall[1]
  accs$sensitivity[i] = CM$byClass[1]
  accs$specificity[i] = CM$byClass[2]
  accs$k[i] = i
}

par(mfrow = c(2,2))
plot(accs$k,accs$accuracy, type = "l", xlab = "K")
plot(accs$k,accs$sensitivity, type = "l", xlab = "K")
plot(accs$k,accs$specificity, type = "l", xlab = "K")
```

## Final Model KNN
After pushing through the process of adding and removing variables, I came up with the final model. The final three factors that created the most accurate model and were thus most influential to attrition in this model were DailyRate, MonthlyIncome, and MonthlyRate. However, as you can see from the statistics, this model was identical to the initial KNN model. This would indicate these are the main three factors that influenced the model. 

```{r Final Model}
#most accurate KNN model
classifications = knn(train[,c(5,20,21)],test[,c(5,20,21)],train$Attrition, prob = TRUE, k = 11)
table(test$Attrition,classifications)
confusionMatrix(table(test$Attrition,classifications),positive = "Yes")
```

## NaiveBayes
While the statistics from the KNN were not completely useless, the model struggled with sensitivity. For this reason, I tested NaiveBayes as a possible alternative to the simple KNN model. NaiveBayes is a machine learning model based on a simple equation that looks at probabilities of a variable to classify an observation. This can be expanded out to many variables to create a more accurate model by adding more probabilities to the base equation. My first approach was to have all variables in a base model to get a baseline metric. I then created a model based on the strongest indicators from the KNN model to see if this would render a more accurate model than the baseline.

```{r NaiveBayes Initial Model}
#naiveBayes with all possible variables to use as a baseline
model = naiveBayes(Attrition ~ Age + BusinessTravel + DailyRate + Department + DistanceFromHome + Education + EducationField + EnvironmentSatisfaction + Gender + HourlyRate + JobInvolvement + JobLevel + JobRole + JobSatisfaction + MaritalStatus + MonthlyIncome + MonthlyRate + NumCompaniesWorked + OverTime + PercentSalaryHike + PerformanceRating + RelationshipSatisfaction + StandardHours + StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager, data = train)
CM = confusionMatrix(table(predict(model,test),test$Attrition), positive = "Yes")
CM

#testing strongest indicators from KNN model
model = naiveBayes(Attrition ~ DailyRate + MonthlyIncome + MonthlyRate, data = train)
CM = confusionMatrix(table(predict(model,test),test$Attrition), positive = "Yes")
CM
```

## Improving NaiveBayes accuracy
After running the strongest indicators from the KNN model and the sensitivity metric coming back as 0, I went back to the base model. I then removed variables and noted their change on the model. Those that, when removed, caused the metrics to decline were kept in the model. Those that, when removed, either improved the model or had no impact were removed. Through this testing the following results were found:
 
#### Top Changers of Model
 
##### Top 4 Accuracy changers
* OverTime
* JobInvolvement
* JobSatisfaction
* MaritalStatus

##### Top 4 Sensitivity Changers
* OverTime
* JobInvolvement
* MaritalStatus
* YearsInCurrentRole

##### Top 4 Sensitivity Changers
* OverTime
* JobSatisfaction
* MaritalStatus
* MonthlyRate

This testing resulted in the final model found below.
```{r Final NaiveBayes Model}
#Final naiveBayes model
model = naiveBayes(Attrition ~ Age + BusinessTravel + DistanceFromHome + Education + EnvironmentSatisfaction + HourlyRate + JobInvolvement + JobLevel + JobSatisfaction + MaritalStatus + MonthlyIncome + MonthlyRate + NumCompaniesWorked + OverTime+ StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager, data = train)
CM = confusionMatrix(table(predict(model,test),test$Attrition), positive = "Yes")
CM
```

## Prediction of Attrition
When comparing the final models for both KNN and NaiveBayes, NaiveBayes was very clearly the more appropriate model for this data set. For this reason it was used to predict Attrition based on a test set supplied by the "client". Below is the code for exporting the predictions as directed. These have been uploaded to the github repository for this case study.
```{r Prediction Attrition}
#Final naiveBayes model - Prediction of Attrition
Case2PredictionsXXXXAttrition_raw <- read.csv('https://raw.githubusercontent.com/nedeinlein/CaseStudy2DDS/main/CaseStudy2CompSet%20No%20Attrition.csv')
model = naiveBayes(Attrition ~ Age + BusinessTravel + DistanceFromHome + Education + EnvironmentSatisfaction + HourlyRate + JobInvolvement + JobLevel + JobSatisfaction + MaritalStatus + MonthlyIncome + MonthlyRate + NumCompaniesWorked + OverTime+ StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager, data = train)
Attrition <- predict(model,Case2PredictionsXXXXAttrition_raw)
Predictions <- cbind(Case2PredictionsXXXXAttrition_raw,Attrition) %>% select(ID,Attrition)
Predictions
```

## Salary Regression
The second task given was to see if a regression model could be fitted to the data set to predict an employee's salary. My initial EDA into this was looking at scatter plots of the data to try to find variables, categorical and continuous, that looked like they might be correlated to MonthlyIncome. Once I had my list of variables, I ran single regression tests against each variable individually against MonthlyIncome to determine which had the strongest correlations.
```{r Initial Regression Testing}
#Finding correlations for Salary individual regressions
fit1 <- lm(MonthlyIncome~Age, data = train)
fit2 <- lm(MonthlyIncome~Attrition, data = train)
fit3 <- lm(MonthlyIncome~JobLevel, data = train)
fit4 <- lm(MonthlyIncome~JobRole, data = train)
fit5 <- lm(MonthlyIncome~TotalWorkingYears, data = train)
fit6 <- lm(MonthlyIncome~YearsAtCompany, data = train)

#Results to put into table
sum1 <- summary(fit1)
sum2 <- summary(fit2)
sum3 <- summary(fit3)
sum4 <- summary(fit4)
sum5 <- summary(fit5)
sum6 <- summary(fit6)

variable <- c(sum1$call,sum2$call,sum3$call,sum4$call,sum5$call,sum6$call)
adj.r <-c(sum1$adj.r.squared,sum2$adj.r.squared,sum3$adj.r.squared,sum4$adj.r.squared,sum5$adj.r.squared,sum6$adj.r.squared)
results <- cbind(variable,adj.r)
results <- as.data.frame(results)
results
```

## Final Regression Model
After porting the adjusted R-Squared values into the easy to read table, I determined that JobLevel, JobRole, and TotalWorkingYears, would probably be my final model. But first, I tested to make sure that no more accurate model could be created than just using those three variables. This was done by substituting variables and removing variables, but this ended up being the final model based on the RMSE.

```{r Final Regression Model}
#Final regression model
fitness <- lm(MonthlyIncome ~ JobLevel + JobRole + TotalWorkingYears, data = train)
summary(fitness)
pred <- predict(fitness, newdata = test)
rmse(test$MonthlyIncome, pred)
```

## Prediction of Salary
The above model was then used on a test date set supplied by the "client" to predict salary. Below is the code that was used to run the prediction and then export the data in the appropriate format for submission.
```{r Regression Prediction}
#Regression prediction
Case2PredictionsXXXXSalary_raw <- read.csv('https://raw.githubusercontent.com/nedeinlein/CaseStudy2DDS/main/CaseStudy2CompSet%20No%20Salary.csv')
fitness<- lm(MonthlyIncome ~ JobLevel + JobRole + TotalWorkingYears, data = train)
MonthlyIncome<- predict(fitness,Case2PredictionsXXXXSalary_raw)
Predictionsalary <- cbind(Case2PredictionsXXXXSalary_raw,MonthlyIncome) %>% select(ID,MonthlyIncome)
Predictionsalary
```

# Summary

#### Initial EDA helped locate possible indicators of attrition and helped quickly model salary prediction
#### Three biggest factors that had strong correlations with attrition were:
* Overtime
* JobInvolvement
* MaritalStatus

#### Salary for employees could be calculated on a multilinear regression model using the factors  of JobLevel, JobRole, and TotalWorkingYears
